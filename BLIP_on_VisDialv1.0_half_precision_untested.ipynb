{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f506ecd",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "### Precision change - done\n",
    "### Image pre-processed size, \n",
    "### Number of max tokens, \n",
    "### Gradient accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6e0a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f44e3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a22e100",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aef107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dc3f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeba7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "model = BlipForQuestionAnswering.from_pretrained(\"ybelkada/blip-vqa-base\", torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6e0a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_visdial = datasets.load_dataset('jxu124/visdial', split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaf6dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_visdial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3ebab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_visdial = ds_visdial.remove_columns('global_image_id')\n",
    "ds_visdial = ds_visdial.remove_columns('anns_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c300d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_visdial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41066497",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = ds_visdial['image_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9531f82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog = ds_visdial['dialog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feac510",
   "metadata": {},
   "outputs": [],
   "source": [
    "capList = ds_visdial['caption']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e6a192",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(img_list), len(dialog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cbdf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ds_visdial['image_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0366044f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_visdial['image_path'][82770:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3e0628",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dialog), len(img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367f9e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57de9045",
   "metadata": {},
   "outputs": [],
   "source": [
    "questionList = []\n",
    "ansList = []\n",
    "img_path_list = []\n",
    "captionList = []\n",
    "\n",
    "for i in tqdm(range(123287)):\n",
    "    \n",
    "    dialogSet = dialog[i]\n",
    "    img = img_list[i]\n",
    "    caption = capList[i]\n",
    "    \n",
    "    for j in range(10):\n",
    "        question = dialogSet[j][0]\n",
    "        answer = dialogSet[j][1]\n",
    "        \n",
    "        questionList.append(question)\n",
    "        ansList.append(answer)\n",
    "        img_path_list.append(img)\n",
    "        captionList.append(caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9776edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "listToDictionary = {'caption':captionList, 'questions':questionList, 'answers':ansList, 'images':img_path_list}\n",
    "modified_train_set = Dataset.from_dict(listToDictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52acd9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05de5ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_visdial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8097a0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_map = {\"coco/val2014\": \"coco/train2014\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3dc6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ReplaceImagePath():\n",
    "    path_map: {}\n",
    "    def __call__(self, features):\n",
    "        print(path_map)\n",
    "        for k, v in self.path_map.items():\n",
    "            features['images'] = features['images'].replace(k, v)\n",
    "        return features\n",
    "    \n",
    "modified_train_set = modified_train_set.map(ReplaceImagePath(path_map=path_map)).cast_column(\"images\", datasets.Image())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6887c07",
   "metadata": {},
   "source": [
    "### Training  Set Ready Now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18eb66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_train_set = modified_train_set.shuffle(seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072b7fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf97fbcd",
   "metadata": {},
   "source": [
    "### Processing Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3231b5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_validation_visdial = datasets.load_dataset('jxu124/visdial', split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d4128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_validation_visdial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0d116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_validation_visdial = ds_validation_visdial.remove_columns('global_image_id')\n",
    "ds_validation_visdial = ds_validation_visdial.remove_columns('anns_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcef528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_validation_visdial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b82c918",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog = ds_validation_visdial['dialog']\n",
    "img_list = ds_validation_visdial['image_path']\n",
    "capList = ds_validation_visdial['caption']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed8b30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "questionList_val = []\n",
    "ansList_val = []\n",
    "img_path_list_val = []\n",
    "captionList_val = []\n",
    "\n",
    "for i in tqdm(range(2064)):\n",
    "    \n",
    "    dialogSet = dialog[i]\n",
    "    img = img_list[i]\n",
    "    caption = capList[i]\n",
    "    \n",
    "    for j in range(10):\n",
    "        question = dialogSet[j][0]\n",
    "        answer = dialogSet[j][1]\n",
    "        \n",
    "        questionList_val.append(question)\n",
    "        ansList_val.append(answer)\n",
    "        img_path_list_val.append(img)\n",
    "        captionList_val.append(caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c4f144",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(questionList_val), len(ansList_val), len(img_path_list_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bcafc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "listToDictionary = {'caption':captionList_val, 'questions':questionList_val, 'answers':ansList_val, 'images':img_path_list_val}\n",
    "modified_val_set = Dataset.from_dict(listToDictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1b36b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7d7cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_val_set = modified_val_set.cast_column(\"images\", datasets.Image())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e799420f",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efa2eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_val_set = modified_val_set.shuffle(seed=42)\n",
    "temp = shuffled_val_set[0:200]\n",
    "small_val_set = Dataset.from_dict(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd37e0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_val_set['questions']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a9708d",
   "metadata": {},
   "source": [
    "##### End processing Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4642f396",
   "metadata": {},
   "source": [
    "#### Finetuning Code Starts Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a37bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22b4fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class visdial_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset, processor):\n",
    "        self.processor = processor\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        encodings = self.processor(images = item[\"images\"], text = item[\"questions\"], padding = \"max_length\", return_tensors = \"pt\")\n",
    "        labels = self.processor(text = item['answers'], padding = \"max_length\", return_tensors = \"pt\").input_ids\n",
    "\n",
    "        encodings['labels'] = labels\n",
    "        encodings = {k:v.squeeze() for k,v in encodings.items()}\n",
    "\n",
    "        return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26de7180",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_object = visdial_dataset(modified_train_set, processor)\n",
    "train_dataloader = DataLoader(train_dataset_object, shuffle=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a60fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "for k,v in batch.items():\n",
    "  print(k, v.shape)\n",
    "\n",
    "print()\n",
    "print(batch.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f973c9a7",
   "metadata": {},
   "source": [
    "### GPU Memory Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24632bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "size = torch.cuda.mem_get_info()\n",
    "test = list(map(lambda x:round(x/(1024*1024*1024),2),size))\n",
    "status = {'available':test[0], 'total':test[1]}\n",
    "status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cf0155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be added\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4637886f",
   "metadata": {},
   "source": [
    "### All Function Declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e56ad73",
   "metadata": {},
   "source": [
    "#### Func to Calculate BLEURT Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65255eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Elron/bleurt-base-512\")\n",
    "bleurt_model = AutoModelForSequenceClassification.from_pretrained(\"Elron/bleurt-base-512\")to(device, torch.float16)\n",
    "bleurt_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc2e469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction and groundTruth variable are expected to contain strings.\n",
    "\n",
    "def bleurt_score(prediction, groundTruth):\n",
    "    references = prediction\n",
    "    candidates = groundTruth\n",
    "\n",
    "    with torch.no_grad():\n",
    "      scores = bleurt_model(**tokenizer(references, candidates, return_tensors='pt'))[0].squeeze()\n",
    "\n",
    "    return(scores) # tensor([1.0327, 0.2055])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0ea9be",
   "metadata": {},
   "source": [
    "#### Func to Evaluate Val Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4bb716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_validation_loss_on_val_set(small_val_set):\n",
    "    \n",
    "    groundTruthList = small_val_set['answers']\n",
    "    predictedList = []\n",
    "    bleurt_scores = []\n",
    "    \n",
    "    # set the model to inference mode\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for i in range(len(small_val_set)):\n",
    "        \n",
    "        question = small_val_set[i]['questions']\n",
    "        image = small_val_set[i]['images']\n",
    "                \n",
    "        inputs = processor(image, question, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        outputs = model.generate(**inputs)\n",
    "        ans = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # print(f'{question}? -> {ans}')\n",
    "\n",
    "        predictedList.append(ans)\n",
    "        \n",
    "    for i in range(len(predictedList)):\n",
    "        \n",
    "        scores = bleurt_score(predictedList[i], groundTruthList[i])\n",
    "        bleurt_scores.append(scores)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    return (sum(bleurt_scores)/len(bleurt_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecfb727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it means 12,32,870 records has been grouped in batch of size 16.\n",
    "# so total number of batch formed is 1232870 / 16 = 77055\n",
    "# that means in each epoch, total the loop will repeat for 77,055 times\n",
    "\n",
    "tot_number_of_steps = len(train_dataloader)\n",
    "number_of_epoch = 10\n",
    "\n",
    "tot_number_of_steps,number_of_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78823170",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c8bc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_loss = 0.0\n",
    "gradient_accumulation_steps = 5\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 5e-5)\n",
    "model.train()\n",
    "\n",
    "for epoch in tqdm(range(number_of_epoch)):\n",
    "\n",
    "# Total 12,00,000 data points are there, keeping a batch size of 16 would make total 12,00,000/16 = 75000 batches\n",
    "# So the below loop will repeat for 75000 times.\n",
    "\n",
    "  for idx, batch in enumerate(train_dataloader):\n",
    "    \n",
    "    input_ids = batch.pop(\"input_ids\").to(device, torch.float16)\n",
    "    pixel_values = batch.pop(\"pixel_values\").to(device, torch.float16)\n",
    "    labels = batch.pop(\"labels\").to(device, torch.float16)\n",
    "    attention_mask = batch.pop('attention_mask').to(device, torch.float16)\n",
    "    \n",
    "    outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    pixel_values=pixel_values,\n",
    "                    labels=labels,\n",
    "                    attention_mask = attention_mask\n",
    "                   )\n",
    "    \n",
    "    loss = outputs.loss\n",
    "\n",
    "    # print(\"Loss:\", loss.item())\n",
    "    print(idx+1 ,end = ' ')\n",
    "    running_loss = running_loss + loss.item()\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # tensorboard display\n",
    "    # here (epoch * tot_number_of_steps + i) denotes the current step out of the total (77054 * num_of_epoch) steps.\n",
    "    # And, running_loss/100 is the mean loss of 100 iterations.\n",
    "    \n",
    "    if (idx+1) % 100 == 0:\n",
    "        \n",
    "        print(f'\\n Epoch:{epoch+1}, Step:{(idx+1)/tot_number_of_steps}, Output Loss:{loss.item()} \\n')\n",
    "        \n",
    "        val_loss = evaluate_validation_loss_on_val_set(small_val_set)\n",
    "        writer.add_scalar('Training Loss', running_loss/100, epoch * tot_number_of_steps + idx)\n",
    "        writer.add_scalar('Validation Loss', val_loss, epoch * tot_number_of_steps + idx)\n",
    "        \n",
    "        running_loss = 0\n",
    "    \n",
    "    # save model at last few epoch\n",
    "    \n",
    "    if epoch in (8,9,10):\n",
    "        save_path = os.path.join(\"./model_chkpts/\", \"mod\" + str(epoch))\n",
    "        model.save_pretrained(save_path)\n",
    "\n",
    "writer.close()\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52c5e6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406c4feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c66695",
   "metadata": {},
   "source": [
    "For BLEURT code docs\n",
    "\n",
    "#### Ref: https://huggingface.co/Elron/bleurt-base-512?text=I+like+you.+I+love+you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99c87bc",
   "metadata": {},
   "source": [
    "For setting up tensorboard and validation loss code position sample\n",
    "\n",
    "#### Ref: https://www.youtube.com/watch?v=VJW9wU-1n18&ab_channel=PatrickLoeber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86052207",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
